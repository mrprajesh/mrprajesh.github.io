<!DOCTYPE html><html lang="en" > <script src="https://cdn.jsdelivr.net/npm/anchor-js/anchor.min.js"></script><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Adaptive cpp (formerly hipSYCL) to run NVIDIA GPUs" /><meta name="author" content="Rajesh Pandian M" /><meta property="og:locale" content="en_US" /><meta name="description" content="I strongly think heterogeneous programming with be the future. AdaptiveCpp was very much along those line – it C++-based heterogeneous programming models targeting CPUs and GPUs from all major vendors!" /><meta property="og:description" content="I strongly think heterogeneous programming with be the future. AdaptiveCpp was very much along those line – it C++-based heterogeneous programming models targeting CPUs and GPUs from all major vendors!" /><link rel="canonical" href="https://mrprajesh.co.in/blog/adaptive-cpp.html" /><meta property="og:url" content="https://mrprajesh.co.in/blog/adaptive-cpp.html" /><meta property="og:site_name" content="Rajesh’s Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-04-16T02:38:17+05:30" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Adaptive cpp (formerly hipSYCL) to run NVIDIA GPUs" /><meta name="twitter:site" content="@mrprajesh" /><meta name="twitter:creator" content="@Rajesh Pandian M" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Rajesh Pandian M"},"dateModified":"2024-04-16T02:38:17+05:30","datePublished":"2024-04-16T02:38:17+05:30","description":"I strongly think heterogeneous programming with be the future. AdaptiveCpp was very much along those line – it C++-based heterogeneous programming models targeting CPUs and GPUs from all major vendors!","headline":"Adaptive cpp (formerly hipSYCL) to run NVIDIA GPUs","mainEntityOfPage":{"@type":"WebPage","@id":"https://mrprajesh.co.in/blog/adaptive-cpp.html"},"url":"https://mrprajesh.co.in/blog/adaptive-cpp.html"}</script><title> Adaptive cpp (formerly hipSYCL) to run NVIDIA GPUs - Rajesh&#39;s Blog</title><link rel="shortcut icon" href="/blog/favicon.png"><link rel="alternate" type="application/atom+xml" title="Rajesh's Blog" href="/blog/atom.xml"><link rel="alternate" type="application/json" title="Rajesh's Blog" href="https://mrprajesh.co.in/blog/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/blog/sitemap.xml" /><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}#footer{padding-bottom:2em;text-align:center;clear:both;width:80%;margin:20px auto} .anchorjs-link { color: #aaa; } .anchorjs-link:hover { color: #ff5231; }</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Rajesh's Blog</h1>--><nav role="navigation"><ul><li><a href="/blog/" >My Blog</a></li><li><a href="/blog/about" >About</a></li><li><a href="/blog/search" >Search</a></li><li><a href="/blog/tags" >Tags</a></li><li><a href="/blog/atom.xml" >RSS</a></li></ul></nav></header><section class="post"><link rel="stylesheet" type="text/css" href="/blog/perldoc.css"><h2>Adaptive cpp (formerly hipSYCL) to run NVIDIA GPUs</h2><blockquote><p>I strongly think heterogeneous programming with be the future. <strong>AdaptiveCpp</strong> was very much along those line – it C++-based heterogeneous programming models targeting CPUs and GPUs from all major vendors!</p></blockquote><blockquote><p>However, <strong>Optimizing</strong> for specific or generic hardware will the challenge to yield peak performance.</p></blockquote><p><strong>Warning</strong>. This is a notes for my later reading. Not an clear document!</p><h3 id="prereq">Prereq</h3><ol><li>Assumes default CUDA install location</li><li>Nvidia GPU</li><li>Understand <a href="https://github.com/AdaptiveCpp/AdaptiveCpp/blob/develop/doc/compilation.md#summary-of-supported-compilation-targets">TARGETS</a> and <a href="https://github.com/AdaptiveCpp/AdaptiveCpp/blob/develop/doc/installing.md#advanced-installation">LLVM versioning</a>, also see doc/install*md compilation.md</li><li>LLVM installed using <a href="#llvm">this steps</a></li><li><code class="language-plaintext highlighter-rouge">generic</code> - creates a binary that can run on all backends – creates the fastest binaries.</li><li><blockquote><p>If you use a very recent CUDA version, you might get a warning when compiling with AdaptiveCpp that clang does not support your CUDA version and treats like an older version. This warning can usually safely be ignored.</p></blockquote></li></ol><h2 id="update">Update</h2><ul><li>WORKING OPTION: <code class="language-plaintext highlighter-rouge">cuda-nvcxx</code> NVIDIA GPUs Library-only</li><li>TRYING! <code class="language-plaintext highlighter-rouge">cuda.integrated-multipass</code> NVIDIA GPUs</li></ul><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># GET IT

git clone https://github.com/AdaptiveCpp/AdaptiveCpp adap-cpp
cd AdaptiveCpp
mkdir build install
cd build

cmake -DCMAKE_INSTALL_PREFIX=`pwd`/install .. # default deductions

cmake -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda -DWITH_CUDA_BACKEND=ON -DCMAKE_INSTALL_PREFIX=`pwd`/install .. # explicit cuda. it is picking llvm 14 instead of 17

cmake -DCMAKE_INSTALL_PREFIX=/home/rajesh/temp/adap-cpp/install  -DLLVM_DIR=/usr/lib/llvm-17/cmake  -DCLANG_INCLUDE_PATH=/usr/lib/llvm-17/lib/clang/17/include   ..  # my try llvm 17; mention explicit?
make -j4  install


cmake -DCMAKE_INSTALL_PREFIX=`pwd`/install  -DLLVM_DIR=/home/rajesh/install/llvm15-src-install/lib/cmake  -DCLANG_INCLUDE_PATH=/home/rajesh/install/llvm15-src-install/include   ..
</code></pre></div></div><h3 id="to-test">to test</h3><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat test.cpp # https://github.com/AdaptiveCpp/AdaptiveCpp/blob/develop/doc/examples.md
# /home/rajesh/temp/adap-cpp/install/bin/acpp test.cpp -o test.out -O3 --acpp-targets=generic

#include &lt;cassert&gt;
#include &lt;iostream&gt;

#include &lt;CL/sycl.hpp&gt;

using data_type = float;

std::vector&lt;data_type&gt; add(cl::sycl::queue&amp; q,
                           const std::vector&lt;data_type&gt;&amp; a,
                           const std::vector&lt;data_type&gt;&amp; b)
{
  std::vector&lt;data_type&gt; c(a.size());

  assert(a.size() == b.size());
  cl::sycl::range&lt;1&gt; work_items{a.size()};

  {
    cl::sycl::buffer&lt;data_type&gt; buff_a(a.data(), a.size());
    cl::sycl::buffer&lt;data_type&gt; buff_b(b.data(), b.size());
    cl::sycl::buffer&lt;data_type&gt; buff_c(c.data(), c.size());

    q.submit([&amp;](cl::sycl::handler&amp; cgh){
      auto access_a = buff_a.get_access&lt;cl::sycl::access::mode::read&gt;(cgh);
      auto access_b = buff_b.get_access&lt;cl::sycl::access::mode::read&gt;(cgh);
      auto access_c = buff_c.get_access&lt;cl::sycl::access::mode::write&gt;(cgh);

      cgh.parallel_for&lt;class vector_add&gt;(work_items,
                                         [=] (cl::sycl::id&lt;1&gt; tid) {
        access_c[tid] = access_a[tid] + access_b[tid];
      });
    });
  }
  return c;
}

int main()
{
  cl::sycl::queue q;
  std::vector&lt;data_type&gt; a = {1.f, 2.f, 3.f, 4.f, 5.f};
  std::vector&lt;data_type&gt; b = {-1.f, 2.f, -3.f, 4.f, -5.f};
  auto result = add(q, a, b);

  std::cout &lt;&lt; "Result: " &lt;&lt; std::endl;
  for(const auto x: result)
    std::cout &lt;&lt; x &lt;&lt; std::endl;
}

</code></pre></div></div><h2 id="post-steps">Post steps</h2><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ls -lrt myinstall/bin
total 324
-rwxr-xr-x 1 rajesh rajesh 67339 Apr 16 02:25 syclcc-clang
-rwxr-xr-x 1 rajesh rajesh 67339 Apr 16 02:25 syclcc
-rwxr-xr-x 1 rajesh rajesh 67339 Apr 16 02:25 acpp
-rwxr-xr-x 1 rajesh rajesh 71216 Apr 16 02:35 acpp-hcf-tool
-rwxr-xr-x 1 rajesh rajesh 48040 Apr 16 02:35 acpp-info


/acpp-info 
=================Backend information===================
Loaded backend 0: CUDA
  Found device: NVIDIA GeForce RTX 3060 Laptop GPU
Loaded backend 1: OpenCL
  Found device: 12th Gen Intel(R) Core(TM) i7-12700H
Loaded backend 2: OpenMP
  Found device: hipSYCL OpenMP host device
Loaded backend 3: HIP
  (no devices found)
  
  ...
  
  

## WORKING?
 
/home/rajesh/temp/adap-cpp/install/bin/acpp test.cpp -o test.out -O3 --acpp-targets=generic
 rajesh   g15warrior   ~  
 03:43   $   ./test.out 
'+ptx86' is not a recognized feature for this target (ignoring feature)
'+ptx86' is not a recognized feature for this target (ignoring feature)
'+ptx86' is not a recognized feature for this target (ignoring feature)
[AdaptiveCpp Warning] kernel_cache: This application run has resulted in new binaries being JIT-compiled. This indicates that the runtime optimization process has not yet reached peak performance. You may want to run the application again until this warning no longer appears to achieve optimal performance.
Result: 
0
4
0
8
0


## LOOKS LIKE SOMETHING IS WRONG

/home/rajesh/temp/adap-cpp/install/bin/acpp test.cpp -o test.out -O3 --acpp-targets="cuda"
/home/rajesh/temp/adap-cpp/install/bin/acpp test.cpp -o test.out -O3 --acpp-targets="cuda"
/home/rajesh/temp/adap-cpp/install/bin/acpp test.cpp -o test.out -O3 --acpp-targets="cuda:sm_70"
clang: warning: CUDA version is newer than the latest supported version 11.5 [-Wunknown-cuda-version]
In file included from &lt;built-in&gt;:1:
In file included from /usr/lib/llvm-14/lib/clang/14.0.0/include/__clang_cuda_runtime_wrapper.h:365:
/usr/lib/llvm-14/lib/clang/14.0.0/include/__clang_cuda_texture_intrinsics.h:696:13: error: no template named 'texture'
            texture&lt;__DataT, __TexT, cudaReadModeNormalizedFloat&gt; __handle,
            ^
/usr/lib/llvm-14/lib/clang/14.0.0/include/__clang_cuda_texture_intrinsics.h:709:13: error: no template named 'texture'
            texture&lt;__DataT, __TexT, cudaReadModeElementType&gt; __handle,
            ^
2 errors generated when compiling for sm_70.


## Let's try only nvc++
   

# https://github.com/AdaptiveCpp/AdaptiveCpp/blob/b61a18683cb66734b3d6a1d02ab3e3cb6f1d7d8d/.github/workflows/linux.yml#L172C9-L173C8

cmake \
-DNVCXX_COMPILER=/opt/nvidia/hpc_sdk/Linux_x86_64/24.3/compilers/bin/nvc++  \
-DWITH_CUDA_BACKEND=ON -DWITH_CUDA_NVCXX_ONLY=ON \
-DCMAKE_INSTALL_PREFIX=`pwd`/install \
-DCUDA_TOOLKIT_ROOT_DIR=/opt/nvidia/hpc_sdk/Linux_x86_64/24.3/cuda/12.3 ..    
 
home/rajesh/temp/adap-cpp/build-nvcpp/install/bin/acpp test.cpp -o test.out -O3 --acpp-targets="cuda-nvcxx" &amp;&amp; ./test.out # great works!

Result: 
0
4
0
8
0
WORKS!!!

# below did not work
/home/rajesh/temp/adap-cpp/build-nvcpp/install/bin/acpp test.cpp -o test.out -O3 --acpp-targets="generic" &amp;&amp; ./test.out #Gives same warnings as before!
/home/rajesh/temp/adap-cpp/build-nvcpp/install/bin/acpp test.cpp -o test.out -O3 --acpp-targets="cuda:sm_70" &amp;&amp; ./test.out # same error 1

 
## if above fails, check with llvm 17 

https://github.com/AdaptiveCpp/AdaptiveCpp/blob/b61a18683cb66734b3d6a1d02ab3e3cb6f1d7d8d/.github/workflows/linux.yml#L84C179-L84C201 

cmake -DCMAKE_CXX_COMPILER=/usr/bin/clang++-17 -DCLANG_EXECUTABLE_PATH=/usr/bin/clang++-17 -DLLVM_DIR=/usr/lib/llvm-17/cmake -DWITH_CUDA_BACKEND=ON -DWITH_ROCM_BACKEND=ON -DCMAKE_INSTALL_PREFIX=`pwd`/install -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda -DROCM_PATH=/opt/rocm ..
    
./install/bin/acpp test.cpp -o test.out -O3 --acpp-targets="cuda-nvcxx" --acpp-nvcxx=/opt/nvidia/hpc_sdk/Linux_x86_64/24.3/compilers/bin/nvc++
./test.out 
Result: 
0
4
0
8
0
16

WOW! nvc++ is the only way out!?


15 did not work
cmake -DCMAKE_CXX_COMPILER=/usr/bin/clang++-15 -DCLANG_EXECUTABLE_PATH=/usr/bin/clang++-15 -DLLVM_DIR=/usr/lib/llvm-15/cmake -DWITH_CUDA_BACKEND=ON -DWITH_ROCM_BACKEND=ON -DCMAKE_INSTALL_PREFIX=`pwd`/install -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda -DROCM_PATH=/opt/rocm ..

./install/bin/acpp test.cpp -o test.out -O3 --acpp-targets="cuda:sm_70"

 
## if fails, build llvm from source
https://github.com/AdaptiveCpp/AdaptiveCpp/issues/745
https://github.com/AdaptiveCpp/AdaptiveCpp/blob/b61a18683cb66734b3d6a1d02ab3e3cb6f1d7d8d/doc/install-llvm.md

git clone https://github.com/llvm/llvm-project -b release/15.x
cd llvm-project
mkdir -p build
cd build

INSTALL_PREFIX=/path/to/desired/llvm/installation/directory

cmake -DCMAKE_C_COMPILER=`which gcc` -DCMAKE_CXX_COMPILER=`which g++` \
      -DCMAKE_BUILD_TYPE=Release \
      -DCMAKE_INSTALL_PREFIX=$INSTALL_PREFIX \
      -DLLVM_ENABLE_PROJECTS="clang;compiler-rt;lld;openmp" \
      -DOPENMP_ENABLE_LIBOMPTARGET=OFF \
      -DCMAKE_BUILD_TYPE=Release \
      -DLLVM_ENABLE_ASSERTIONS=OFF \
      -DLLVM_TARGETS_TO_BUILD="NVPTX;X86" \  
      -DCLANG_ANALYZER_ENABLE_Z3_SOLVER=0 \
      -DLLVM_INCLUDE_BENCHMARKS=0 \
      -DLLVM_INCLUDE_EXAMPLES=0 \
      -DLLVM_INCLUDE_TESTS=0 \
      -DCMAKE_INSTALL_RPATH_USE_LINK_PATH=ON \
      -DCMAKE_INSTALL_RPATH=$INSTALL_PREFIX/lib \
      -DLLVM_ENABLE_OCAMLDOC=OFF \
      -DLLVM_ENABLE_BINDINGS=OFF \
      -DLLVM_TEMPORARILY_ALLOW_OLD_TOOLCHAIN=OFF \
      -DLLVM_BUILD_LLVM_DYLIB=ON \
      -DLLVM_ENABLE_DUMP=OFF  ../llvm

make -j8 install



## 

</code></pre></div></div><h2 id="llvm">LLVM</h2><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://apt.llvm.org/llvm.sh #Convenience script that sets up the repositories
chmod +x llvm.sh

sudo ./llvm.sh 17 #Set up repositories for clang 17
sudo apt install -y libclang-17-dev clang-tools-17 libomp-17-dev llvm-17-dev lld-17


</code></pre></div></div><h3 id="errors-while-wip">Errors while WIP</h3><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## error-0
/home/rajesh/temp/adap-cpp/build-cuda/install/bin/acpp test.cpp -o test.out -O3 --acpp-targets="cuda"
acpp error: fatal: No CUDA targets specified

## error-1
/home/rajesh/temp/adap-cpp/build-nvcpp/install/bin/acpp test.cpp -o test.out -O3 --acpp-targets="cuda:sm_70"
clang: warning: CUDA version is newer than the latest supported version 11.5 [-Wunknown-cuda-version]
In file included from &lt;built-in&gt;:1:
In file included from /usr/lib/llvm-14/lib/clang/14.0.0/include/__clang_cuda_runtime_wrapper.h:365:
/usr/lib/llvm-14/lib/clang/14.0.0/include/__clang_cuda_texture_intrinsics.h:696:13: error: no template named 'texture'
            texture&lt;__DataT, __TexT, cudaReadModeNormalizedFloat&gt; __handle,
            ^
/usr/lib/llvm-14/lib/clang/14.0.0/include/__clang_cuda_texture_intrinsics.h:709:13: error: no template named 'texture'
            texture&lt;__DataT, __TexT, cudaReadModeElementType&gt; __handle,
            ^
In file included from &lt;built-in&gt;:1:
/usr/lib/llvm-14/lib/clang/14.0.0/include/__clang_cuda_runtime_wrapper.h:486:10: fatal error: 'curand_mtgp32_kernel.h' file not found
#include "curand_mtgp32_kernel.h"
         ^~~~~~~~~~~~~~~~~~~~~~~~
3 errors generated when compiling for sm_70.

## error-2
 /home/rajesh/temp/adap-cpp/build-llvm17/install/bin/acpp test.cpp -o test.out -O3 --acpp-targets="cuda.integrated-multipass:sm_70"
clang++-17: warning: CUDA version is newer than the latest partially supported version 12.1 [-Wunknown-cuda-version]
In file included from &lt;built-in&gt;:1:
In file included from /usr/lib/llvm-17/lib/clang/17/include/__clang_cuda_runtime_wrapper.h:111:
In file included from /usr/local/cuda/include/cuda_runtime.h:82:
/usr/local/cuda/include/crt/host_config.h:151:2: error: -- unsupported clang version! clang version must be less than 16 and greater than 3.2 . The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.
  151 | #error -- unsupported clang version! clang version must be less than 16 and greater than 3.2 . The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.
      |  ^
1 error generated when compiling for sm_70.

## errror 3
 /home/rajesh/temp/adap-cpp/build-llvm17/install/bin/acpp test.cpp -o test.out -O3 --acpp-targets="cuda:sm_70" --acpp-cuda-path=/opt/nvidia/hpc_sdk/Linux_x86_64/24.3/cuda
clang++-17: warning: CUDA version is newer than the latest partially supported version 12.1 [-Wunknown-cuda-version]
In file included from &lt;built-in&gt;:1:
In file included from /usr/lib/llvm-17/lib/clang/17/include/__clang_cuda_runtime_wrapper.h:111:
In file included from /opt/nvidia/hpc_sdk/Linux_x86_64/24.3/cuda/include/cuda_runtime.h:82:
/opt/nvidia/hpc_sdk/Linux_x86_64/24.3/cuda/include/crt/host_config.h:151:2: error: -- unsupported clang version! clang version must be less than 16 and greater than 3.2 . The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.
  151 | #error -- unsupported clang version! clang version must be less than 16 and greater than 3.2 . The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.
      |  ^
In file included from &lt;built-in&gt;:1:
/usr/lib/llvm-17/lib/clang/17/include/__clang_cuda_runtime_wrapper.h:486:10: fatal error: 'curand_mtgp32_kernel.h' file not found
  486 | #include "curand_mtgp32_kernel.h"
      |          ^~~~~~~~~~~~~~~~~~~~~~~~
2 errors generated when compiling for sm_70.


-- &gt; IF BUILD SRC then we should go for LLVM 15?
https://github.com/AdaptiveCpp/AdaptiveCpp/blob/develop/doc/install-llvm.md

/home/rajesh/install/llvm-project/build

INSTALL_PREFIX=/home/rajesh/install/llvm15-src-install


cmake -DCMAKE_INSTALL_PREFIX=`pwd`/install  -DLLVM_CMAKE_DIR=/home/rajesh/install/llvm15-src-install/lib/cmake  -DCLANG_INCLUDE_PATH=/home/rajesh/install/llvm15-src-install/include/  .. 

LLVM_ROOT did not work
cmake/llvm # only builds but can not complie test

cmake -DCMAKE_CXX_COMPILER=/home/rajesh/install/llvm15-src-install/bin/clang++ -DCLANG_EXECUTABLE_PATH=/home/rajesh/install/llvm15-src-install/bin/clang++ -DLLVM_DIR=/home/rajesh/install/llvm15-src-install/lib/cmake/llvm -DWITH_CUDA_BACKEND=ON -DWITH_ROCM_BACKEND=ON -DCMAKE_INSTALL_PREFIX=`pwd`/install -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda -DROCM_PATH=/opt/rocm ..

MABE TRY LLVM 16 src
https://github.com/AdaptiveCpp/AdaptiveCpp/issues/1013


</code></pre></div></div><span class="meta"> ★ 8 min read &middot; <a href="/" > Rajesh Pandian M</a> &middot; <time datetime="2024-04-16T02:38:17+05:30">16-Apr-2024 02:38:17 (IST)</time> <!-- date: "%d-%b-%Y %T %Z (UTC%z)" --> ★ <a href="/blog/tag/cuda"> cuda </a> , <a href="/blog/tag/adaptivecpp"> adaptivecpp </a> , <a href="/blog/tag/acpp"> acpp </a> , <a href="/blog/tag/sycl"> sycl </a> , <a href="/blog/tag/hipSYCL"> hipSYCL </a> </span> <script src="https://utteranc.es/client.js" repo="mrprajesh/blog" issue-term="pathname" label="comments" theme="github-light" crossorigin="anonymous" async> </script> <!--<span class="meta"><time datetime="2024-04-16T02:38:17+05:30">April 16, 2024</time> &middot; <a class="post" href="/tag/cuda">cuda</a>, <a class="post" href="/tag/adaptivecpp">adaptivecpp</a>, <a class="post" href="/tag/acpp">acpp</a>, <a class="post" href="/tag/sycl">sycl</a>, <a class="post" href="/tag/hipSYCL">hipSYCL</a></span> --></section></main><footer id="footer" class="body"><p> Made with ❤ <a href="https://jekyllrb.com//">Jekyll</a> &nbsp;&nbsp; &nbsp; | &nbsp; Customized <a href="https://github.com/ronv/sidey"> «Sidey» </a> theme &nbsp; | &nbsp; Fork/Star <a href="https://github.com/mrprajesh/blog">on GitHub </a></p><p id="LDate">Last Modified On: Monday, 20 May 2024 01:18:09 IST</p></footer><script> anchors.options.placement = 'left'; anchors.add(); </script></body></html>
